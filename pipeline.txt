PIPELINE

===============================================================================
0. BEFORE FIRST USE
===============================================================================

a)  Software prerequisites

    i)  MySQL 5.6 or later

        sudo apt-get install mysql-server-5.6 mysql-server-core-5.6 \
            mysql-client-5.6 mysql-client-core-5.6

b)  Ensure that all source database(s) are accessible to the Processing
    Computer.

c)  Write a draft config file, giving connection details for all source
    databases.

d)  Ensure that the PYTHONPATH is pointing to necessary Python support files.

e)  Ensure that the data dictionary has been created, and then updated and
    verified by a human.
    Use "anonymise.py -d <configfile>" to generate a draft config.
    Edit it with any TSV editor (e.g. Excel, LibreOffice Calc).

f)  Install GATE:
    - Download GATE Developer
    - java -jar gate-8.0-build4825-installer.jar
    - Read the documentation; it's quite good.

===============================================================================
1. PRE-PROCESSING
===============================================================================

a)  Ensure that the databases are copied and ready.

b)  Add in any additional data. For example, if you want to process a postcode
    field to geographical output areas, such as
        http://en.wikipedia.org/wiki/ONS_coding_system
    then do it now; add in the new fields. Don't remove the old (raw) postcodes;
    they'll be necessary for anonymisation.

c)  UNCOMMON OPTION: anonymise using NLP to find names. See below.
    If you want to anonymise using NLP to find names, rather than just use the
    name information in your source database, run nlp_manager.py now, using
    (for example) the Person annotation from GATE's
        plugins/ANNIE/ANNIE_with_defaults.gapp
    application, and send the output back into your database. You'll need to
    ensure the resulting data has patient IDs attached, probably with a view
    (see (d) below).

d)  Ensure every table that relates to a patient has a common field with the
    patient ID that's used across the database(s) to be anonymised.
    Create views if necessary. The data dictionary should reflect this work.

e)  Strongly consider using a row_id (e.g. integer primary key) field for each
    table. This will make natural language batch processing simpler (see
    below).

f)  For extracting text from other things (DOC, DOCX, ODT, PDF):
        extract_text.py
    This script sends its output to stdout, but is easily modifiable to send
    it elsewhere, or you can redirect its output in a calling script; it's also
    easily parallelized.

===============================================================================
2. ANONYMISATION (AND FULL-TEXT INDEXING) USING A DATA DICTIONARY
===============================================================================

OBJECTIVES:
    - Make a record-by-record copy of tables in the source database(s).
      Handle tables that do and tables that don't contain patient-identifiable
      information.
    - Collect patient-identifiable information and use it to "scrub" free-text
      fields; for example, with forename=John, surname=Smith, and spouse=Jane,
      one can convert freetext="I saw John in clinic with Sheila present" to
      "I saw XXX in clinic with YYY present" in the output. Deal with date,
      numerical, textual, and number-as-text information sensibly.
    - Allow other aspects of information restriction, e.g. truncating dates of
      birth to the first of the month.
    - Apply one-way encryption to patient ID numbers (storing a secure copy for
      superuser re-identification).
    - Enable linking of data from multiple source databases with a common
      identifier (such as the NHS number), similarly encrypted.
    - For performance reasons, enable parallel processing and incremental
      updates.

    For help: anonymise.py --help

a)  METHOD 1: THREAD-BASED. THIS IS SLOWER.
        anonymise.py <configfile> [--threads=<n>]

b)  METHOD 2: PROCESS-BASED. THIS IS FASTER.
    See example in launch_multiprocess.sh

    ---------------------------------------------------------------------------
    Work distribution
    ---------------------------------------------------------------------------
    - Best performance from multiprocess (not multithreaded) operation.
    - Drop/rebuild tables: single-process operation only.
    - Non-patient tables:
        - if there's an integer PK, split by row
        - if there's no integer PK, split by table (in sequence of all tables).
    - Patient tables: split by patient ID.
      (You have to process all scrubbing information from a patient
      simultaneously, so that's the unit of work. Patient IDs need to be
      integer for this method, though for no other reason.)
    - Indexes: split by table (in sequence of all tables requiring indexing).
      (Indexing a whole table at a time is fastest, not index by index.)

    ---------------------------------------------------------------------------
    Incremental updates
    ---------------------------------------------------------------------------
    - Supported via the --incremental option.
    - The problems include:
        - aspects of patient data (e.g. address/phone number) might, in a
          very less than ideal world, change rather than being added to. How
          to detect such a change?
        - If a new phone number is added (even in a sensible way) -- or, more
          importantly, a new alias (following an anonymisation failure),
          should re-scrub all records for that patient, even records previously
          scrubbed.
    - Solution:
        - Only tables with a suitable PK can be processed incrementally.
          The PK must appear in the destination database (and therefore can't
          be sensitive, but should be an uninformative integer).
          This is so that if a row is deleted from the source, one can check
          by looking at the destination.
        - For a table with a src_pk, one can set the add_src_hash flag.
          If set, then a hash of all source fields (more specifically: all that
          are not omitted from the destination, plus any that are used for
          scrubbing, i.e. scrubsrc_patient or scrubsrc_thirdparty) is created
          and stored in the destination database.
        - Let's call tables that use the src_pk/add_src_hash system "hashed"
          tables.
        - During incremental processing:
            1. Non-hashed tables are dropped and rebuilt entirely.
               Any records in a hashed destination table that don't have a
               matching PK in their source table are deleted.
            2. For each patient, the scrubber is calculated. If the
               *scrubber's* hash has changed (stored in the secret_map table),
               then all destination records for that patient are reworked
               in full (i.e. the incremental option is disabled for that
               patient).
            3. During processing of a table (either per-table for non-patient
               tables, or per-patient then per-table for patient tables), each
               row has its source hash recalculated. For a non-hashed table,
               this is then reprocessed normally. For a hashed table, if there
               is a record with a matching PK and a matching source hash, that
               record is skipped.

    ---------------------------------------------------------------------------
    Multiple databases
    ---------------------------------------------------------------------------
    - The intention is that if you anonymise multiple databases together,
      then they must share a patient numbering (ID) system. For example, you
      might have two databases using RiO numbers; you can anonymise them
      together. If they also have an NHS number, that can be hashed as a master
      PID, for linking to other databases (anonymised separately). (If you used
      the NHS number as the primary PID, the practical difference would be that
      you would ditch any patients who have a RiO number but no NHS number
      recorded.)
    - Each databases must each use a consistent name for this field, across all
      tables, WITHIN that database..
    - This field, which must be an integer, must fit into a BIGINT UNSIGNED
      field (see wipe_and_recreate_mapping_table() in anonymise.py).
    - However, the databases don't have to use the same *name* for the field.
      For example, RiO might use "id" to mean "RiO number", while CamCOPS might
      use "_patient_idnum1".
    - A scrubber will be built across all source databases, which may improve
      anonymisation.

===============================================================================
3. NATURAL LANGUAGE PROCESSING
===============================================================================

OBJECTIVES: Send free-text content to natural language processing (NLP) tools,
storing the results in structured form in a relational database -- for example,
to find references to people, drugs/doses, cognitive examination scores, or
symptoms.

    - For help: nlp_manager.py --help
    - The Java element needs building; use buildjava.sh

    - STRUCTURE: see nlp_manager.py; CamAnonGatePipeline.java

    - Run the Python script in parallel; see launch_multiprocess_nlp.sh

    ---------------------------------------------------------------------------
    Work distribution
    ---------------------------------------------------------------------------
    - Parallelize by source_pk.

    ---------------------------------------------------------------------------
    Incremental updates
    ---------------------------------------------------------------------------
    - Here, incremental updates are simpler, as the NLP just requires a record
      taken on its own.
    - Nonetheless, still need to deal with the conceptual problem of source
      record modification; how would we detect that?
        - One method would be to hash the source record, and store that with
          the destination...
    - Solution:
        1. Delete any destination records without a corresponding source.
        2. For each record, hash the source.
           If a destination exists with the matching hash, skip.

===============================================================================
EXTRA: ANONYMISATION USING NLP.
===============================================================================

OBJECTIVE: remove other names not properly tagged in the source database.

Here, we have a preliminary stage. Instead of the usual:

                        free text
    source database -------------------------------------> anonymiser
                |                                           ^
                |                                           | scrubbing
                +-------------------------------------------+ information


we have:

                        free text
    source database -------------------------------------> anonymiser
          |     |                                           ^  ^
          |     |                                           |  | scrubbing
          |     +-------------------------------------------+  | information
          |                                                    |
          +---> NLP software ---> list of names ---------------+
                                  (stored in source DB
                                   or separate DB)

For example, you could:

    a) run the NLP processor to find names, feeding its output back into a new
       table in the source database, e.g. with these options:

            inputfielddefs =
                SOME_FIELD_DEF
            outputtypemap =
                person SOME_OUTPUT_DEF
            progenvsection = SOME_ENV_SECTION
            progargs = java
                -classpath {NLPPROGDIR}:{GATEDIR}/bin/gate.jar:{GATEDIR}/lib/*
                CamAnonGatePipeline
                -g {GATEDIR}/plugins/ANNIE/ANNIE_with_defaults.gapp
                -a Person
                -it END_OF_TEXT_FOR_NLP
                -ot END_OF_NLP_OUTPUT_RECORD
                -lt {NLPLOGTAG}
            input_terminator = END_OF_TEXT_FOR_NLP
            output_terminator = END_OF_NLP_OUTPUT_RECORD

            # ...

    b) add a view to include patient numbers, e.g.

            CREATE VIEW patient_nlp_names
            AS SELECT
                notes.patient_id,
                nlp_person_from_notes._content AS nlp_name
            FROM notes
            INNER JOIN nlp_person_from_notes
                ON notes.note_id = nlp_person_from_notes._srcpkval
            ;

    c) feed that lot to the anonymiser, including the NLP-generated names as
       scrubsrc_* field(s).


===============================================================================
4. SQL ACCESS
===============================================================================

OBJECTIVE: research access to the anonymised database(s).

a)  Grant READ-ONLY access to the output database for any relevant user.

b)  Don't grant any access to the secret mapping database! This is for
    trusted superusers only.

c)  You're all set.

===============================================================================
5. FRONT END (SQL BUILDER) FOR NON-SQL AFICIONADOS
===============================================================================

??? Python web site script, reading from data dictionary, etc.

    - Script knows field type, from data dictionary.
    - Collapsible list. Comments.
    - Possible operators include:

        <   <=  =   =>  >
                !=

    - For strings: =    LIKE / MATCH(x) AGAINST (...)

    - SELECT outputfields WHERE constraints
    - preview with COUNT(*) as output

    - automatic joins on patient
      (NOT: separate query)

    - SQL preview
    - ??raw SQL option - danger of SQL injection

... all too much effort for what you get? Everyone doing it seriously will use
SQL.

===============================================================================
X. TROUBLESHOOTING
===============================================================================

-------------------------------------------------------------------------------
Q.  Error: [Microsoft][SQL Server Native Client 11.0]Connection is busy with
    results for another command.
-------------------------------------------------------------------------------
A.  In /etc/odbc.ini, for that DSN, set
        MARS_Connection = yes
    - https://msdn.microsoft.com/en-us/library/cfa084cz(v=vs.110).aspx
    - https://msdn.microsoft.com/en-us/library/h32h3abf(v=vs.110).aspx
    - Rationale: We use gen_patient_ids() to iterate through patients, but then
     we fetch data for that patient via the same connection to the source
     database(s). Therefore, we're operating multiple result sets through one
     connection.

-------------------------------------------------------------------------------
Q.  How to convert from SQL Server to MySQL?
-------------------------------------------------------------------------------
A.  MySQL Workbench.
    Use the "ODBC via connection string" option if others aren't working:
        DSN=XXX;UID=YYY;PWD=ZZZ
    If the schema definitions are not seen, it's a permissions issue:
        http://stackoverflow.com/questions/17038716
    ... but you can copy the database using anonymise.py, treating all tables
    as non-patient tables (i.e. doing no actual anonymisation).

-------------------------------------------------------------------------------
Q.  MySQL server has gone away... ?
-------------------------------------------------------------------------------
A.  Probably: processing a big binary field, and MySQL's max_allowed_packet
    parameter is too small. Try increasing it (e.g. from 16M to 32M).
    See also http://www.camcops.org/documentation/server.html

-------------------------------------------------------------------------------
Q.  What settings do I need in /etc/mysql/my.cnf?
-------------------------------------------------------------------------------
A.  Probably these:

    [mysqld]
    max_allowed_packet = 32M

    innodb_strict_mode = 1
    innodb_file_per_table = 1
    innodb_file_format = Barracuda

    # Only for MySQL prior to 5.7.5 (http://dev.mysql.com/doc/relnotes/mysql/5.6/en/news-5-6-20.html):
    innodb_log_file_size = 320M

    # For more performance, less safety:
    innodb_flush_log_at_trx_commit = 2

    # To save memory?
    # Default is 8; suggestion is ncores * 2
    # innodb_thread_concurrency = ...

    [mysqldump]
    max_allowed_packet = 32M

-------------------------------------------------------------------------------
Q.  _mysql_exceptions.OperationalError: (1118, 'Row size too large (> 8126).
    Changing some columns to TEXT or BLOB or using ROW_FORMAT=DYNAMIC or
    ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of 768
    bytes is stored inline.')
-------------------------------------------------------------------------------
A.  See above.
    If you need to change the log file size, FOLLOW THIS PROCEDURE:
        https://dev.mysql.com/doc/refman/5.0/en/innodb-data-log-reconfiguration.html

-------------------------------------------------------------------------------
Q.  Segmentation fault (core dumped)... ?
-------------------------------------------------------------------------------
A.  Short answer: use the Microsoft JDBC driver instead of the Microsoft ODBC
    driver for Linux, which is buggy.

    Long answer, i.e. working this out:

    Examine the core with gdb anonymise.py ~/core
    ... then it tells you which program generated the core
    ... then gdb PROGRAM ~/core
    ... but actually the likely reason is being out of RAM
    ... monitor memory use with
            htop
            top (press M)
            watch free -m
                http://www.linuxatemyram.com/
    ... tried: reduce the innodb_thread_concurrency variable as above, and
        restart MySQL (under Ubuntu/Debian, with: sudo service mysql restart).
        - didn't fix it
    ... for 32M max_allowed_packet, use 320M (not 512M) for the logfile
        - did significantly reduce memory usage, but still crashed, and not
          while processing a large record
        - longest BLOB in this data set is
    So, systematic method:
    (1) What's the biggest packet needed? Estimate with:
            SELECT MAX(LEN(giantbinaryfield)) FROM relevanttable;
        ... in our case (CRS/CDL test): 39,294,299 = 37.47 MiB.
        So with a bit of margin, let's use
            max_allowed_packet = 40M
            innodb_log_file_size = 400M
    (2) Then set max number of rows and bytes, e.g. to 1000 rows and 80 MiB.
    OK, actually relates to a single specific record -- found using MySQL
    log with
            SET GLOBAL general_log = 'ON';
            SHOW VARIABLES LIKE 'general_log_file';
    ... but actually not relating to insertion at all, but to retrieval
    ... nrows=90060 then crash in gen_rows at the point of cursor.fetchone()
    ... This?
        http://stackoverflow.com/questions/11657958
        https://code.google.com/p/pyodbc/issues/detail?id=346
        https://msdn.microsoft.com/en-us/library/hh568448.aspx
        https://code.google.com/p/pyodbc/issues/detail?id=188
    ... changing rnc_db to use pypyodbc rather than pyodbc:
            sudo pip install pypyodbc
            import pypyodbc as pyodbc
        ... crashed at the same point (segfault).
        ... so back to pyodbc
    ... git clone https://github.com/mkleehammer/pyodbc
        ... getdata.cpp, as one bughunt above suggested, already has that fix
    ... sudo pip install pyodbc --upgrade  # from 3.0.6 to 3.0.7
        ... no change
    ... try the query using Perl and DBI::ODBC -- also crashes.
        So probably a bug in the SQL Server Native Client 11.0 for Linux.
    ... can't use FreeTDS because the SQL Server won't let us login (another
        Microsoft bug).
    ... removing the VARCHAR(MAX) fields from the data dictionary makes it happy again.
    ... random: http://www.saltycrane.com/blog/2011/09/notes-sqlalchemy-w-pyodbc-freetds-ubuntu/

    [Full details in private log.]

    Switched to the JDBC driver.
    Problem went away.

-------------------------------------------------------------------------------
Q.  "Killed."
-------------------------------------------------------------------------------
A.  Out of memory.
    Testing on a rather small machine (0.5 Gb RAM, 1 Gb swap).
    Inspect what was running:

        # cat /var/log/syslog

    Remove memory-hogging things:

        # apt-get purge modemmanager
        - change the report_crashes parameter to false in the /etc/default/whoopsie file.
        # service whoopsie stop
        # apt-get remove unity unity-asset-pool unity-control-center unity-control-center-signon unity-gtk-module-common unity-lens* unity-services unity-settings-daemon unity-webapps* unity-voice-service
        ... NOT YET REMOVED: network-manager

    Inspect it:

        # pmap -x <process_id>

    Leaks?
    - http://www.lshift.net/blog/2008/11/14/tracing-python-memory-leaks/

        $ python -m pdb ./anonymise.py
        (Pdb) run crs_cdl_anon.ini -v
        (Pdb) c

    Use openDBcopy to copy the database: http://opendbcopy.sourceforge.net/

        Prerequisites
            export JAVA_HOME=/usr/lib/jvm/default-java
            cd ~/openDBcopy/bin
            ./start.sh &

        Plugin chain:

            - Migrate database schema (DDL)

                0.  Configuration

                1.  Database connections
                    SOURCE
                        Driver name = Microsoft MSSQL Server JDBC Driver
                        Driver class = com.microsoft.sqlserver.jdbc.SQLServerDriver
                        URL = jdbc:sqlserver://XXX:1433;databaseName=XXX
                        User name = XXX
                        Password = XXX
                    DESTINATION
                        Driver name = MySQL Driver
                        Driver class = com.mysql.jdbc.Driver
                        URL = jdbc:mysql://localhost:3306/DATABASENAME
                        User name = XXX
                        Password = XXX
                    TEST BOTH.

                2.  Source model
                        Catalog = [DATABASE NAME]
                        Schema = dbo
                        Table pattern = %
                    CAPTURE SOURCE MODEL.

                3.  Tables to migrate
                        = all by default

                4.  Columns to migrate
                        = all by default

            - Copy data from a source into a destination database

        ... NOT WORKING.

    - http://stackoverflow.com/questions/27068092/jpype-java-initialize-with-or-get-remaining-heap-space

    - http://stackoverflow.com/questions/1178736/mysql-maximum-memory-usage
    - SHOW ENGINE INNODB STATUS

    USEFUL THINGS:
    - see estimate_mysql_memory_usage.sh
    - changed innodb_buffer_pool_size from 128M to 16M
        ... big improvement; mysqld %MEM (in top) went from ~30% to ~10%
    - RTF processing takes lots of memory, using Python/pyth
        ... significant improvement after switching to Linux/unrtf
        ... similarly, using Linux/pdftotext rather than Python/pdfminer

    AFTERWARDS:
    - Show table size and number of rows in MySQL:

        SELECT table_name AS "Table",
            ROUND(((data_length + index_length) / 1024 / 1024), 2) AS "Size in MiB",
            table_rows
        FROM information_schema.TABLES
        WHERE table_schema = DATABASE()
        ORDER BY (data_length + index_length) DESC;

      or:

        SELECT table_name AS "Table",
            ROUND(((data_length + index_length) / 1024 / 1024), 2) AS "Size in MiB",
            table_rows
        FROM information_schema.TABLES
        WHERE table_schema = DATABASE()
        ORDER BY table_name;

    TEMPORARY HOLDUP: not enough disk space (~9.2 Gb on CPFT test machine):

        +---------------------+-------------+------------+
        | Table               | Size in MiB | table_rows |
        +---------------------+-------------+------------+
        | address             |       63.61 |     431262 |
        | alias               |        5.52 |      58468 |
        | assessment          |      256.63 |       9725 |
        | careplan            |      191.64 |      16801 |
        | careplangoal        |       98.64 |     187922 |
        | cdlinternalreferral |        2.52 |       4679 |
        | cdlpatient          |        2.52 |      14014 |
        | cgas                |        1.52 |       2571 |
        | dependant           |        0.13 |       1001 |
        | diagnosis           |        8.52 |      76361 |
        | documentlibrary     |     3795.00 |     474874 |
        | employment_status   |        0.02 |          0 |
        | exclude             |        0.02 |          0 |
        | honos               |        0.02 |          0 |
        | honos_65            |        0.02 |          0 |
        | honos_ca            |        0.02 |          0 |
        | honos_ld            |        0.02 |          0 |
        | honos_secure        |        0.02 |          0 |
        | living_arrangements |        0.02 |          0 |
        | mpi                 |        0.02 |          0 |
        | personal_carers     |        0.02 |          0 |
        | practicegp          |        0.02 |          0 |
        | procedures          |        0.02 |          0 |
        | referral            |        0.02 |          0 |
        | schedules           |        0.02 |          0 |
        | team_episodes       |        0.02 |          0 |
        | telephone           |        0.02 |          0 |
        | ward_stays          |        0.02 |          0 |
        +---------------------+-------------+------------+
        28 rows in set (0.42 sec)

        ... THEN OUT OF DISK SPACE:

        _mysql_exceptions.OperationalError: (1114, "The table 'documentlibrary' is full")
